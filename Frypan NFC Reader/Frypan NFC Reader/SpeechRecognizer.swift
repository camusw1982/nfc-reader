//
//  SpeechRecognizer.swift
//  Frypan NFC Reader
//
//  Created by Claude on 5/9/2025.
//

import Foundation
import Speech
import SwiftUI
import AVFoundation
import Combine

// ChatMessage å·²ç§»è‡³ ChatComponents.swift

class SpeechRecognizer: NSObject, ObservableObject, SFSpeechRecognizerDelegate {
    @Published var isRecognizing = false
    @Published var recognizedText = ""
    @Published var hasPermission = false
    @Published var error: String?
    @Published var lastSentText: String?
    @Published var llmResponse: String = ""
    @Published var originalText: String = ""
    @Published var responseTimestamp: Date?
    @Published var isWebSocketConnected = false
    @Published var isRecordingCancelled = false
    
    // å°è©±æ¶ˆæ¯æ•¸çµ„
    @Published var messages: [ChatMessage] = []
    
    let webService = WebServiceManager()
    
    private let speechRecognizer: SFSpeechRecognizer?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    private let audioSession = AVAudioSession.sharedInstance()
    
    override init() {
        self.speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-HK"))
        super.init()
        speechRecognizer?.delegate = self
        requestPermission()
        setupWebSocketMonitoring()
    }
    
    func requestPermission() {
        SFSpeechRecognizer.requestAuthorization { [weak self] status in
            DispatchQueue.main.async {
                switch status {
                case .authorized:
                    self?.hasPermission = true
                    print("âœ… èªéŸ³è­˜åˆ¥æ¬Šé™å·²æˆäºˆ")
                case .denied:
                    self?.error = "èªéŸ³è­˜åˆ¥æ¬Šé™è¢«æ‹’çµ•"
                    print("âŒ èªéŸ³è­˜åˆ¥æ¬Šé™è¢«æ‹’çµ•")
                case .restricted:
                    self?.error = "èªéŸ³è­˜åˆ¥åŠŸèƒ½å—é™"
                    print("âŒ èªéŸ³è­˜åˆ¥åŠŸèƒ½å—é™")
                case .notDetermined:
                    self?.error = "èªéŸ³è­˜åˆ¥æ¬Šé™æœªç¢ºå®š"
                    print("âŒ èªéŸ³è­˜åˆ¥æ¬Šé™æœªç¢ºå®š")
                @unknown default:
                    self?.error = "æœªçŸ¥çš„æ¬Šé™ç‹€æ…‹"
                    print("âŒ æœªçŸ¥çš„æ¬Šé™ç‹€æ…‹")
                }
            }
        }
    }
    
    func startRecording() {
        guard hasPermission else {
            error = "æ²’æœ‰èªéŸ³è­˜åˆ¥æ¬Šé™"
            return
        }
        
        guard !isRecognizing else {
            print("èªéŸ³è­˜åˆ¥å·²åœ¨é€²è¡Œä¸­")
            return
        }
        
        // å–æ¶ˆä¹‹å‰çš„ä»»å‹™
        if let recognitionTask = recognitionTask {
            recognitionTask.cancel()
            self.recognitionTask = nil
        }
        
        // é…ç½®éŸ³é »æœƒè©± - ä½¿ç”¨æ›´å®‰å…¨çš„é…ç½®
        do {
            try audioSession.setCategory(.playAndRecord, mode: .default, options: [.defaultToSpeaker, .allowBluetooth])
            try audioSession.setActive(true)
        } catch let audioError {
            self.error = "éŸ³é »æœƒè©±é…ç½®å¤±æ•—: \(audioError.localizedDescription)"
            return
        }
        
        // å‰µå»ºè­˜åˆ¥è«‹æ±‚
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            error = "ç„¡æ³•å‰µå»ºè­˜åˆ¥è«‹æ±‚"
            return
        }
        
        recognitionRequest.shouldReportPartialResults = true
        
        // é–‹å§‹è­˜åˆ¥ä»»å‹™
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            
            if let result = result {
                let transcribedString = result.bestTranscription.formattedString
                DispatchQueue.main.async {
                    self.recognizedText = transcribedString
                    // åªåœ¨è­˜åˆ¥å®Œæˆæ™‚è¼¸å‡ºæ—¥èªŒï¼Œä¸åœ¨æ¯æ¬¡éƒ¨åˆ†çµæœæ™‚è¼¸å‡º
                    if result.isFinal {
                        print("ğŸ¤ èªéŸ³è­˜åˆ¥å®Œæˆ: \(transcribedString)")
                    }
                }
            }
            
            if let error = error {
                let errorDescription = error.localizedDescription
                // éæ¿¾æ‰æ­£å¸¸æƒ…æ³çš„éŒ¯èª¤
                if errorDescription == "No speech detected" {
                    print("â„¹ï¸ æœªæª¢æ¸¬åˆ°èªéŸ³ (æ­£å¸¸æƒ…æ³)")
                } else if errorDescription == "Recognition request was canceled" {
                    print("â„¹ï¸ èªéŸ³è­˜åˆ¥è«‹æ±‚å·²å–æ¶ˆ (æ­£å¸¸æƒ…æ³)")
                } else {
                    DispatchQueue.main.async {
                        self.error = "è­˜åˆ¥éŒ¯èª¤: \(errorDescription)"
                        print("âŒ èªéŸ³è­˜åˆ¥éŒ¯èª¤: \(errorDescription)")
                    }
                }
            }
        }
        
        // é…ç½®éŸ³é »è¼¸å…¥
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { [weak self] buffer, _ in
            self?.recognitionRequest?.append(buffer)
        }
        
        // å•Ÿå‹•éŸ³é »å¼•æ“
        do {
            audioEngine.prepare()
            try audioEngine.start()
            DispatchQueue.main.async {
                self.isRecognizing = true
                self.error = nil
                print("ğŸ¤ èªéŸ³è­˜åˆ¥å·²é–‹å§‹")
            }
        } catch let engineError {
            self.error = "éŸ³é »å¼•æ“å•Ÿå‹•å¤±æ•—: \(engineError.localizedDescription)"
            stopRecording()
        }
    }
    
    func stopRecording() {
        stopRecording(shouldSendResult: false)
    }
    
    func stopRecording(shouldSendResult: Bool) {
        if isRecognizing {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
            
            // å…ˆçµæŸéŸ³é »è«‹æ±‚ï¼Œè€Œä¸æ˜¯å–æ¶ˆä»»å‹™
            recognitionRequest?.endAudio()
            recognitionRequest = nil
            
            // ç­‰å¾…ä¸€å°æ®µæ™‚é–“è®“è­˜åˆ¥ä»»å‹™è‡ªç„¶å®Œæˆ
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
                // åªæœ‰åœ¨ä»»å‹™é‚„å­˜åœ¨æ™‚æ‰å–æ¶ˆ
                if self.recognitionTask != nil {
                    self.recognitionTask?.cancel()
                    self.recognitionTask = nil
                }
                
                // åœç”¨éŸ³é »æœƒè©±
                do {
                    try self.audioSession.setActive(false)
                } catch {
                    print("âš ï¸ åœç”¨éŸ³é »æœƒè©±å¤±æ•—: \(error.localizedDescription)")
                }
                
                self.isRecognizing = false
                print("ğŸ›‘ èªéŸ³è­˜åˆ¥å·²åœæ­¢")
                
                // å¦‚æœæœ‰è­˜åˆ¥çµæœä¸”éœ€è¦ç™¼é€ï¼Œä¸”éŒ„éŸ³æœªè¢«å–æ¶ˆï¼Œç™¼é€åˆ°æœå‹™å™¨
                if shouldSendResult && !self.recognizedText.isEmpty && !self.isRecordingCancelled {
                    self.sendToServer()
                }
                
                // é‡ç½®å–æ¶ˆæ¨™èªŒ
                self.isRecordingCancelled = false
            }
        }
    }
    
    private func sendToServer() {
        // ğŸ”’ å®‰å…¨æªæ–½ï¼šå‘¢å€‹æ–¹æ³•å””æ‡‰è©²å†è¢«èª¿ç”¨
        print("âš ï¸ sendToServer() è¢«èª¿ç”¨ï¼Œä½†æ ¹æ“šæ–°é‚è¼¯æ‡‰è©²ç›´æ¥ä½¿ç”¨ sendTextToSpeech")
        print("ğŸ“¤ æ‹’çµ•ç™¼é€èˆŠæ ¼å¼è«‹æ±‚ï¼Œè«‹ä½¿ç”¨ sendTextToSpeech æ–¹æ³•")
        
        // å””ç™¼é€ä»»ä½•è«‹æ±‚ï¼Œç¢ºä¿åªæœƒé€šé confirmRecording() ç™¼é€ gemini_to_speech
        return
    }
    
    func reset() {
        stopRecording()
        DispatchQueue.main.async {
            self.recognizedText = ""
            self.error = nil
            self.llmResponse = ""
            self.originalText = ""
            self.responseTimestamp = nil
            self.isRecordingCancelled = false
            self.messages.removeAll()
        }
    }
    
    func clearChat() {
        DispatchQueue.main.async {
            self.messages.removeAll()
        }
    }
    
    private func setupWebSocketMonitoring() {
        // ç›£è½ WebSocket é€£æ¥ç‹€æ…‹
        if let webSocketManager = webService.getWebSocketManager() {
            webSocketManager.$isConnected.sink { [weak self] isConnected in
                DispatchQueue.main.async {
                    self?.isWebSocketConnected = isConnected
                    print("ğŸ”Œ WebSocket é€£æ¥ç‹€æ…‹: \(isConnected)")
                }
            }.store(in: &cancellables)
            
            // ç›£è½æ”¶åˆ°çš„æ¶ˆæ¯
            webSocketManager.$receivedMessages.sink { [weak self] messages in
                guard let self = self, let lastMessage = messages.last else { return }
                
                // è§£æ Gemini å›æ‡‰
                if let data = lastMessage.data(using: .utf8),
                   let json = try? JSONSerialization.jsonObject(with: data, options: []) as? [String: Any] {
                    
                    if let type = json["type"] as? String {
                        if type == "response" || type == "gemini_response" {
                            if let response = json["response"] as? String {
                                DispatchQueue.main.async {
                                    self.llmResponse = response
                                    self.responseTimestamp = Date()
                                    print("ğŸ¤– æ”¶åˆ° Gemini å›æ‡‰: \(response)")
                                    
                                    // æ·»åŠ  AI å›æ‡‰åˆ°å°è©±
                                    let aiMessage = ChatMessage(text: response, isUser: false, timestamp: Date(), isError: false)
                                    self.messages.append(aiMessage)
                                }
                            }
                            if let originalText = json["original_text"] as? String {
                                DispatchQueue.main.async {
                                    self.originalText = originalText
                                    print("ğŸ“ åŸå§‹æ–‡æœ¬: \(originalText)")
                                }
                            }
                        } else if type == "pong" {
                            print("ğŸ“ æœå‹™å™¨éŸ¿æ‡‰æ­£å¸¸")
                        } else if type == "history", let history = json["history"] as? [[String: Any]] {
                            print("ğŸ“š æ”¶åˆ°æ­·å²è¨˜éŒ„: \(history.count) æ¢å°è©±")
                        }
                    }
                } else {
                    // å¦‚æœå””ä¿‚ JSON æ ¼å¼ï¼Œç›´æ¥ä½œç‚ºå›æ‡‰è™•ç†
                    DispatchQueue.main.async {
                        self.llmResponse = lastMessage
                        print("ğŸ¤– æ”¶åˆ°æ–‡æœ¬å›æ‡‰: \(lastMessage)")
                        
                        // æ·»åŠ  AI å›æ‡‰åˆ°å°è©±
                        let aiMessage = ChatMessage(text: lastMessage, isUser: false, timestamp: Date(), isError: false)
                        self.messages.append(aiMessage)
                    }
                }
            }.store(in: &cancellables)
        }
    }
    
    // MARK: - Combine è¨‚é–±ç®¡ç†
    private var cancellables = Set<AnyCancellable>()
    
    // MARK: - SFSpeechRecognizerDelegate
    
    func speechRecognizer(_ speechRecognizer: SFSpeechRecognizer, availabilityDidChange available: Bool) {
        DispatchQueue.main.async {
            if available {
                print("âœ… èªéŸ³è­˜åˆ¥å¯ç”¨")
            } else {
                print("âŒ èªéŸ³è­˜åˆ¥ä¸å¯ç”¨")
                self.error = "èªéŸ³è­˜åˆ¥æš«æ™‚ä¸å¯ç”¨"
            }
        }
    }
}